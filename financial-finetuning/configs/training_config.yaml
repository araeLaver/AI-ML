# Financial LLM Fine-tuning Configuration
# ========================================

# Model Settings
model:
  # Base model (Korean-friendly options)
  name: "beomi/Llama-3-Open-Ko-8B"  # Korean Llama 3
  # Alternatives:
  # - "beomi/llama-2-ko-7b"
  # - "Qwen/Qwen2.5-7B-Instruct"
  # - "mistralai/Mistral-7B-Instruct-v0.2"

  max_seq_length: 2048
  dtype: "float16"  # or "bfloat16" for newer GPUs

# LoRA Configuration
lora:
  r: 16                    # Rank (8-64, higher = more capacity)
  lora_alpha: 32           # Scaling factor (usually 2*r)
  lora_dropout: 0.05       # Dropout for regularization
  bias: "none"             # "none", "all", or "lora_only"
  target_modules:          # Modules to apply LoRA
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"

# Quantization (QLoRA)
quantization:
  enabled: true
  load_in_4bit: true
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: true

# Training Settings
training:
  output_dir: "./outputs"
  num_train_epochs: 3
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 2.0e-4
  weight_decay: 0.01
  warmup_ratio: 0.03
  lr_scheduler_type: "cosine"

  # Optimization
  optim: "paged_adamw_8bit"
  fp16: false
  bf16: true

  # Logging
  logging_steps: 10
  save_strategy: "epoch"
  eval_strategy: "epoch"

  # Hub
  push_to_hub: false
  hub_model_id: null

  # Misc
  seed: 42
  report_to: "tensorboard"  # or "wandb"

# Data Settings
data:
  train_file: "./data/processed/train.json"
  eval_file: "./data/processed/eval.json"
  test_size: 0.1

  # Prompt template
  prompt_template: |
    ### 지시사항:
    {instruction}

    ### 입력:
    {input}

    ### 응답:
    {output}

  # For instruction-only (no input)
  prompt_template_no_input: |
    ### 지시사항:
    {instruction}

    ### 응답:
    {output}

# Evaluation Settings
evaluation:
  metrics:
    - "rouge"
    - "bleu"
  num_samples: 100

# Inference Settings
inference:
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  max_new_tokens: 512
  repetition_penalty: 1.1
